{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ef45e4-2d28-4fe5-b842-5da5a4baa4ae",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da239dcb-4bb3-444d-9cf3-9e0b8634482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4aac7-d5c5-4543-8e34-b3067eec85cb",
   "metadata": {},
   "source": [
    "### Define paths Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49200b49-e78e-4238-af39-569ae3090c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base = Path().resolve()\n",
    "data_dir = base / \"Data\"\n",
    "gpkg_path = data_dir / \"Corallinales\" / \"gbif_corallinales_all_20250405\" / \"corallinales_2010_onward.gpkg\"\n",
    "raster_dir = data_dir / \"Bio_oracle\" / \"2010_2020\" / \"Mean_Cropped\"\n",
    "output_path = gpkg_path.parent / \"corallinales_2010_onward_deduplicated.gpkg\"\n",
    "\n",
    "# GbIF paths\n",
    "coralline_files = {\n",
    "    \"2000_2010\": data_dir / \"Corallinales\" / \"gbif_corallinales_all_20250405\" / \"corallinales_2000_2010_deduplicated.gpkg\",\n",
    "    \"2010_onward\": data_dir / \"Corallinales\" / \"gbif_corallinales_all_20250405\" / \"corallinales_2010_onward_deduplicated.gpkg\"\n",
    "}\n",
    "\n",
    "pseudo_absence_combined = {\n",
    "    \"2000_2010\": data_dir /\"Pseudo_absence\" / \"Combined\" / \"comb_2000_2010.gpkg\",\n",
    "    \"2010_onward\": data_dir / \"Pseudo_absence\" / \"Combined\" / \"comb_2010_onward.gpkg\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3de84-653a-4617-b18d-bed44cf8e449",
   "metadata": {},
   "source": [
    "### Reduce datasets to environmental unique points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dec1752-b3a7-4fe2-b7bd-4e00775178c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 285 unique environmental points to C:\\Users\\rasmu\\Desktop\\Universitet\\Maerl\\Data\\Corallinales\\gbif_corallinales_all_20250405\\corallinales_2010_onward_deduplicated.gpkg\n"
     ]
    }
   ],
   "source": [
    "# Define list of selected environmental variable raster filenames\n",
    "selected_vars_tiff_files = [\n",
    "    \"chl\", \"clt\", \"dfe\", \"o2\", \"thetao\",\n",
    "    \"no3\", \"dfe\", \"kdpar\", \"ph\",\n",
    "    \"sws\", \"swd\", \"terrain\"\n",
    "]\n",
    "\n",
    "# Load GeoPackage and reproject to WGS84\n",
    "gdf = gpd.read_file(gpkg_path).to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Initialize empty DataFrame to store sampled values\n",
    "sampled_data = pd.DataFrame(index=gdf.index)\n",
    "\n",
    "# Extract coordinates in (lat, lon) format for sampling\n",
    "coords = [(geom.y, geom.x) for geom in gdf.geometry]\n",
    "\n",
    "# Loop through each selected raster variable\n",
    "for var in selected_vars_tiff_files:\n",
    "    raster_path = raster_dir / f\"{var}.tif\"  # construct raster file path\n",
    "    raster = rxr.open_rasterio(raster_path, masked=True).squeeze()  # load raster and remove singleton dimension\n",
    "\n",
    "    # Sample raster values at each coordinate\n",
    "    samples = []\n",
    "    for lat, lon in coords:\n",
    "        try:\n",
    "            val = raster.sel(y=lat, x=lon, method='nearest').item()  # sample raster using nearest pixel\n",
    "        except Exception:\n",
    "            val = np.nan  # assign NaN if sampling fails\n",
    "        samples.append(val)\n",
    "\n",
    "    sampled_data[var] = samples  # store results for current variable\n",
    "\n",
    "# Create mask to keep only rows with complete data\n",
    "mask = sampled_data.notnull().all(axis=1)\n",
    "gdf_filtered = gdf.loc[mask]  # filter original GeoDataFrame\n",
    "sampled_data = sampled_data.loc[mask]  # filter sampled data\n",
    "\n",
    "# Combine sampled data with spatial geometry\n",
    "sampled_data[\"geometry\"] = gdf_filtered.geometry.values\n",
    "dedup_gdf = gpd.GeoDataFrame(sampled_data, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Drop duplicate combinations of environmental variables\n",
    "dedup_gdf = dedup_gdf.drop_duplicates(subset=selected_vars_tiff_files)\n",
    "\n",
    "# Save filtered and deduplicated GeoDataFrame to GeoPackage\n",
    "dedup_gdf.to_file(output_path, driver=\"GPKG\")\n",
    "print(f\"Saved {len(dedup_gdf)} unique environmental points to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753e8838-c41e-40e1-aa58-e91a0b138993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2000_2010...\n",
      "Target number of observations: 42\n",
      "Saved balanced pseudo absence data to C:\\Users\\rasmu\\Desktop\\Universitet\\Maerl\\Data\\Pseudo_absence\\Combined\\Balanced\\pseudo_absence_balanced_2000_2010.gpkg (42 rows)\n",
      "\n",
      "Processing 2010_onward...\n",
      "Target number of observations: 285\n",
      "Saved balanced pseudo absence data to C:\\Users\\rasmu\\Desktop\\Universitet\\Maerl\\Data\\Pseudo_absence\\Combined\\Balanced\\pseudo_absence_balanced_2010_onward.gpkg (285 rows)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set base path to data directory\n",
    "base = Path(\"C:/Users/rasmu/Desktop/Universitet/Maerl/Data\")\n",
    "\n",
    "# Define input paths for Corallinales occurrence data\n",
    "coralline_files = {\n",
    "    \"2000_2010\": base / \"Corallinales\" / \"gbif_corallinales_all_20250405\" / \"corallinales_2000_2010_deduplicated.gpkg\",\n",
    "    \"2010_onward\": base / \"Corallinales\" / \"gbif_corallinales_all_20250405\" / \"corallinales_2010_onward_deduplicated.gpkg\"\n",
    "}\n",
    "\n",
    "# Define input paths for pseudo-absence datasets\n",
    "pseudo_absence_combined = {\n",
    "    \"2000_2010\": base / \"Pseudo_absence\" / \"Combined\" / \"comb_2000_2010.gpkg\",\n",
    "    \"2010_onward\": base / \"Pseudo_absence\" / \"Combined\" / \"comb_2010_onward.gpkg\"\n",
    "}\n",
    "\n",
    "# Create output directory for balanced pseudo-absence files\n",
    "output_dir = base / \"Pseudo_absence\" / \"Combined\" / \"Balanced\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Function to balance a dataset by equal sampling from unique values in a given column\n",
    "def balance_by_order(gdf, n_total, order_col=\"order\"):\n",
    "    order_list = gdf[order_col].unique()  # get unique group labels\n",
    "    n_per_order = n_total // len(order_list)  # calculate how many to sample from each group\n",
    "    balanced_parts = []\n",
    "\n",
    "    for order in order_list:\n",
    "        subset = gdf[gdf[order_col] == order]  # subset for current group\n",
    "        if len(subset) < n_per_order:\n",
    "            resampled = resample(subset, replace=True, n_samples=n_per_order, random_state=42)  # upsample if needed\n",
    "        else:\n",
    "            resampled = resample(subset, replace=False, n_samples=n_per_order, random_state=42)  # downsample\n",
    "        balanced_parts.append(resampled)  # store resampled part\n",
    "\n",
    "    balanced = pd.concat(balanced_parts).sample(n=n_total, random_state=42)  # combine and shuffle\n",
    "    return gpd.GeoDataFrame(balanced, geometry=\"geometry\", crs=gdf.crs)  # return as GeoDataFrame\n",
    "\n",
    "# Loop through both time periods and process\n",
    "for period in [\"2000_2010\", \"2010_onward\"]:\n",
    "    print(f\"Processing {period}...\")\n",
    "\n",
    "    cor_gdf = gpd.read_file(coralline_files[period])  # load presence points\n",
    "    n_target = len(cor_gdf)  # determine how many absences to match\n",
    "    print(f\"Target number of observations: {n_target}\")\n",
    "\n",
    "    pseudo_gdf = gpd.read_file(pseudo_absence_combined[period])  # load pseudo-absence data\n",
    "\n",
    "    if \"order\" not in pseudo_gdf.columns:  # check for required column\n",
    "        raise ValueError(\"Expected an 'Order' column in pseudo absence dataset.\")\n",
    "\n",
    "    balanced_gdf = balance_by_order(pseudo_gdf, n_total=n_target, order_col=\"order\")  # balance data\n",
    "\n",
    "    output_file = output_dir / f\"pseudo_absence_balanced_{period}.gpkg\"  # define output path\n",
    "    balanced_gdf.to_file(output_file, driver=\"GPKG\")  # save to GeoPackage\n",
    "    print(f\"Saved balanced pseudo absence data to {output_file} ({len(balanced_gdf)} rows)\\n\")  # confirm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
